"""
ì œì•½ ì œì¡° ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤ìŠµ

ì´ ì‹¤ìŠµì—ì„œëŠ” ì œì•½ ì œì¡° ê³µì • ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‚´ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤:
1. ì œì¡° ë°ì´í„° ì²˜ë¦¬
2. ë°ì´í„° ìˆ˜ì§‘ê³¼ ë°ì´í„° ì „ì²˜ë¦¬
3. ì°¨ì›ì¶•ì†Œì™€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
4. ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤ìŠµ

ë°ì´í„°ì…‹: ê³ ìš©ëŸ‰ ì½œë ˆìŠ¤í…Œë¡¤ ì €í•˜ í•„ë¦„-ì½”íŒ… ì •ì œ 1,005 ë°°ì¹˜ì˜ ì‹¤ì œ ì œì¡°Â·ì‹œí—˜ ê¸°ë¡
ê¸°ê°„: 2018ë…„ 11ì›” ~ 2021ë…„ 4ì›”
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.style.use('default')

print("=" * 80)
print("ì œì•½ ì œì¡° ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤ìŠµ")
print("=" * 80)

# =============================================================================
# 1. ë°ì´í„° ìˆ˜ì§‘ ë° ë¡œë”©
# =============================================================================
print("\nğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ë° ë¡œë”©")
print("-" * 50)

def load_pharmaceutical_data():
    """ì œì•½ ì œì¡° ë°ì´í„° ë¡œë”© í•¨ìˆ˜"""
    try:
        # ì‹¤í—˜ì‹¤ ë°ì´í„° (ë°°ì¹˜ë³„ ì›ë£Œ, ì¤‘ê°„ì œí’ˆ, ì™„ì œí’ˆ í’ˆì§ˆ ë°ì´í„°)
        laboratory_df = pd.read_csv('Laboratory.csv', sep=';')
        print(f"âœ… Laboratory ë°ì´í„° ë¡œë”© ì™„ë£Œ: {laboratory_df.shape}")
        
        # ê³µì • ë°ì´í„° (ë°°ì¹˜ë³„ ì§‘ê³„ëœ ê³µì • ì„¼ì„œ ë°ì´í„°)
        process_df = pd.read_csv('Process.csv', sep=';')
        print(f"âœ… Process ë°ì´í„° ë¡œë”© ì™„ë£Œ: {process_df.shape}")
        
        # ì •ê·œí™” ê³„ìˆ˜ ë°ì´í„°
        normalization_df = pd.read_csv('Normalization.csv', sep=';')
        print(f"âœ… Normalization ë°ì´í„° ë¡œë”© ì™„ë£Œ: {normalization_df.shape}")
        
        return laboratory_df, process_df, normalization_df
    
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return None, None, None

# ë°ì´í„° ë¡œë”©
lab_data, process_data, norm_data = load_pharmaceutical_data()

if lab_data is not None:
    print(f"\nğŸ“ˆ ë°ì´í„° ê°œìš”:")
    print(f"- ì´ ë°°ì¹˜ ìˆ˜: {len(lab_data)}")
    print(f"- Laboratory ë³€ìˆ˜ ìˆ˜: {len(lab_data.columns)}")
    print(f"- Process ë³€ìˆ˜ ìˆ˜: {len(process_data.columns)}")

# =============================================================================
# 2. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
# =============================================================================
print("\nğŸ“Š 2ë‹¨ê³„: íƒìƒ‰ì  ë°ì´í„° ë¶„ì„")
print("-" * 50)

def explore_data(lab_df, process_df):
    """ë°ì´í„° íƒìƒ‰ í•¨ìˆ˜"""
    
    print("ğŸ” Laboratory ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
    print(lab_df.info())
    
    print("\nğŸ” Process ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
    print(process_df.info())
    
    # ê²°ì¸¡ê°’ í™•ì¸
    print("\nâ— Laboratory ë°ì´í„° ê²°ì¸¡ê°’:")
    missing_lab = lab_df.isnull().sum()
    print(missing_lab[missing_lab > 0])
    
    print("\nâ— Process ë°ì´í„° ê²°ì¸¡ê°’:")
    missing_process = process_df.isnull().sum()
    print(missing_process[missing_process > 0])
    
    # ê¸°ë³¸ í†µê³„
    print("\nğŸ“Š ì£¼ìš” í’ˆì§ˆ ì§€í‘œ í†µê³„:")
    quality_cols = ['dissolution_av', 'dissolution_min', 'impurities_total']
    if all(col in lab_df.columns for col in quality_cols):
        print(lab_df[quality_cols].describe())

if lab_data is not None:
    explore_data(lab_data, process_data)

# =============================================================================
# 3. ë°ì´í„° ì „ì²˜ë¦¬
# =============================================================================
print("\nğŸ”§ 3ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬")
print("-" * 50)

def preprocess_data(lab_df, process_df, norm_df):
    """ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    
    # ë°ì´í„° ë³µì‚¬
    lab_clean = lab_df.copy()
    process_clean = process_df.copy()
    
    print("ğŸ§¹ 1) ê²°ì¸¡ê°’ ì²˜ë¦¬")
    
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‹ë³„
    numeric_cols_lab = lab_clean.select_dtypes(include=[np.number]).columns
    numeric_cols_process = process_clean.select_dtypes(include=[np.number]).columns
    
    # ê²°ì¸¡ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
    for col in numeric_cols_lab:
        if lab_clean[col].isnull().sum() > 0:
            median_val = lab_clean[col].median()
            lab_clean[col].fillna(median_val, inplace=True)
            print(f"   - {col}: ê²°ì¸¡ê°’ {lab_clean[col].isnull().sum()}ê°œë¥¼ ì¤‘ì•™ê°’ {median_val:.2f}ë¡œ ëŒ€ì²´")
    
    for col in numeric_cols_process:
        if process_clean[col].isnull().sum() > 0:
            median_val = process_clean[col].median()
            process_clean[col].fillna(median_val, inplace=True)
            print(f"   - {col}: ê²°ì¸¡ê°’ {process_clean[col].isnull().sum()}ê°œë¥¼ ì¤‘ì•™ê°’ {median_val:.2f}ë¡œ ëŒ€ì²´")
    
    print("\nğŸ”— 2) ë°ì´í„° ë³‘í•©")
    
    # batch ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
    merged_df = pd.merge(lab_clean, process_clean, on='batch', how='inner')
    print(f"   - ë³‘í•© í›„ ë°ì´í„° í¬ê¸°: {merged_df.shape}")
    
    # ì •ê·œí™” ê³„ìˆ˜ ë³‘í•©
    if 'code_x' in merged_df.columns:
        merged_df = pd.merge(merged_df, norm_df, left_on='code_x', right_on='Product code', how='left')
    
    print("\nğŸš« 3) ì´ìƒê°’ ì œê±°")
    
    # IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒê°’ ì œê±°
    def remove_outliers_iqr(df, columns):
        df_clean = df.copy()
        outliers_removed = 0
        
        for col in columns:
            if col in df_clean.columns and df_clean[col].dtype in ['int64', 'float64']:
                Q1 = df_clean[col].quantile(0.25)
                Q3 = df_clean[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]
                df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
                outliers_removed += len(outliers)
        
        return df_clean, outliers_removed
    
    # ì£¼ìš” í’ˆì§ˆ ì§€í‘œì—ì„œ ì´ìƒê°’ ì œê±°
    quality_indicators = ['dissolution_av', 'dissolution_min', 'impurities_total', 'tbl_speed_mean']
    merged_clean, outliers_count = remove_outliers_iqr(merged_df, quality_indicators)
    print(f"   - ì œê±°ëœ ì´ìƒê°’: {outliers_count}ê°œ")
    print(f"   - ìµœì¢… ë°ì´í„° í¬ê¸°: {merged_clean.shape}")
    
    return merged_clean

if lab_data is not None:
    merged_data = preprocess_data(lab_data, process_data, norm_data)

# =============================================================================
# 4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
# =============================================================================
print("\nâš™ï¸ 4ë‹¨ê³„: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§")
print("-" * 50)

def feature_engineering(df):
    """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜"""
    
    feature_df = df.copy()
    
    print("ğŸ”§ 1) ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±")
    
    # ê³µì • íš¨ìœ¨ì„± ì§€í‘œ
    if 'total_waste' in feature_df.columns and 'batch_yield' in feature_df.columns:
        feature_df['process_efficiency'] = feature_df['batch_yield'] / (feature_df['total_waste'] + 1)
        print("   - process_efficiency: ê³µì • íš¨ìœ¨ì„± ì§€í‘œ ìƒì„±")
    
    # í’ˆì§ˆ ì¼ê´€ì„± ì§€í‘œ
    if 'dissolution_av' in feature_df.columns and 'dissolution_min' in feature_df.columns:
        feature_df['quality_consistency'] = feature_df['dissolution_min'] / feature_df['dissolution_av']
        print("   - quality_consistency: í’ˆì§ˆ ì¼ê´€ì„± ì§€í‘œ ìƒì„±")
    
    # ì••ì¶•ë ¥ ë³€ë™ ì§€í‘œ
    if 'main_CompForce mean' in feature_df.columns and 'main_CompForce_sd' in feature_df.columns:
        feature_df['compression_stability'] = feature_df['main_CompForce_sd'] / feature_df['main_CompForce mean']
        print("   - compression_stability: ì••ì¶•ë ¥ ì•ˆì •ì„± ì§€í‘œ ìƒì„±")
    
    # ì›ë£Œ í’ˆì§ˆ ì¢…í•© ì ìˆ˜
    api_cols = [col for col in feature_df.columns if 'api_' in col and feature_df[col].dtype in ['int64', 'float64']]
    if len(api_cols) > 0:
        feature_df['api_quality_score'] = feature_df[api_cols].mean(axis=1)
        print("   - api_quality_score: API í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ìƒì„±")
    
    print("\nğŸ“Š 2) ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©")
    
    # ë²”ì£¼í˜• ë³€ìˆ˜ ì›-í•« ì¸ì½”ë”©
    categorical_cols = feature_df.select_dtypes(include=['object']).columns
    categorical_cols = [col for col in categorical_cols if col not in ['batch', 'start']]  # ì‹ë³„ì ì œì™¸
    
    for col in categorical_cols:
        if feature_df[col].nunique() < 20:  # ìœ ë‹ˆí¬ ê°’ì´ 20ê°œ ë¯¸ë§Œì¸ ê²½ìš°ë§Œ
            dummies = pd.get_dummies(feature_df[col], prefix=col, drop_first=True)
            feature_df = pd.concat([feature_df, dummies], axis=1)
            feature_df.drop(col, axis=1, inplace=True)
            print(f"   - {col}: ì›-í•« ì¸ì½”ë”© ì™„ë£Œ")
    
    return feature_df

if 'merged_data' in locals():
    engineered_data = feature_engineering(merged_data)
    print(f"\nâœ… íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ - ìµœì¢… íŠ¹ì„± ìˆ˜: {len(engineered_data.columns)}")

# =============================================================================
# 5. ë°ì´í„° ìŠ¤ì¼€ì¼ë§
# =============================================================================
print("\nğŸ“ 5ë‹¨ê³„: ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
print("-" * 50)

def scale_features(df, target_cols=None):
    """íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜"""
    
    if target_cols is None:
        target_cols = ['dissolution_av', 'dissolution_min', 'impurities_total']
    
    # ìˆ˜ì¹˜í˜• íŠ¹ì„±ë§Œ ì„ íƒ
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    feature_cols = [col for col in numeric_cols if col not in target_cols + ['batch']]
    
    X = df[feature_cols].copy()
    y = df[target_cols].copy() if all(col in df.columns for col in target_cols) else None
    
    print(f"ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜: {target_cols}")
    print(f"ğŸ“Š íŠ¹ì„± ë³€ìˆ˜ ìˆ˜: {len(feature_cols)}")
    
    # ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ë§ ë°©ë²• ë¹„êµ
    scalers = {
        'StandardScaler': StandardScaler(),
        'MinMaxScaler': MinMaxScaler(),
        'RobustScaler': RobustScaler()
    }
    
    scaled_data = {}
    
    for name, scaler in scalers.items():
        X_scaled = scaler.fit_transform(X)
        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)
        scaled_data[name] = X_scaled_df
        
        print(f"   âœ… {name} ì ìš© ì™„ë£Œ")
        print(f"      - í‰ê· : {X_scaled.mean():.3f}, í‘œì¤€í¸ì°¨: {X_scaled.std():.3f}")
    
    return scaled_data, X, y, feature_cols

if 'engineered_data' in locals():
    scaled_datasets, original_X, target_y, feature_names = scale_features(engineered_data)

# =============================================================================
# 6. ì°¨ì› ì¶•ì†Œ
# =============================================================================
print("\nğŸ“‰ 6ë‹¨ê³„: ì°¨ì› ì¶•ì†Œ")
print("-" * 50)

def dimension_reduction(X_scaled, n_components=10):
    """ì°¨ì› ì¶•ì†Œ í•¨ìˆ˜"""
    
    print("ğŸ” 1) ì£¼ì„±ë¶„ ë¶„ì„ (PCA)")
    
    # PCA ì ìš©
    pca = PCA()
    X_pca_full = pca.fit_transform(X_scaled)
    
    # ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨ ê³„ì‚°
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)
    
    # 95% ë¶„ì‚°ì„ ì„¤ëª…í•˜ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜ ì°¾ê¸°
    n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1
    print(f"   - 95% ë¶„ì‚° ì„¤ëª…ì— í•„ìš”í•œ ì£¼ì„±ë¶„ ìˆ˜: {n_components_95}")
    
    # ì§€ì •ëœ ê°œìˆ˜ì˜ ì£¼ì„±ë¶„ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ
    pca_reduced = PCA(n_components=min(n_components, X_scaled.shape[1]))
    X_pca_reduced = pca_reduced.fit_transform(X_scaled)
    
    print(f"   - ì„ íƒëœ ì£¼ì„±ë¶„ ìˆ˜: {pca_reduced.n_components_}")
    print(f"   - ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ë¹„ìœ¨: {pca_reduced.explained_variance_ratio_.sum():.3f}")
    
    # PCA ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(range(1, min(21, len(explained_variance_ratio)) + 1), 
             explained_variance_ratio[:20], 'bo-')
    plt.xlabel('Principal Component')
    plt.ylabel('Explained Variance Ratio')
    plt.title('Explained Variance by Principal Component')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(range(1, min(21, len(cumulative_variance_ratio)) + 1), 
             cumulative_variance_ratio[:20], 'ro-')
    plt.axhline(y=0.95, color='k', linestyle='--', alpha=0.7)
    plt.xlabel('Number of Principal Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('Cumulative Explained Variance')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('pca_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return X_pca_reduced, pca_reduced, n_components_95

if 'scaled_datasets' in locals():
    # StandardScalerë¡œ ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì‚¬ìš©
    X_scaled = scaled_datasets['StandardScaler']
    X_pca, pca_model, optimal_components = dimension_reduction(X_scaled)

# =============================================================================
# 7. íŠ¹ì„± ì„ íƒ
# =============================================================================
print("\nğŸ¯ 7ë‹¨ê³„: íŠ¹ì„± ì„ íƒ")
print("-" * 50)

def feature_selection(X, y, feature_names, k=20):
    """íŠ¹ì„± ì„ íƒ í•¨ìˆ˜"""
    
    if y is None or len(y.columns) == 0:
        print("âŒ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì—†ì–´ íŠ¹ì„± ì„ íƒì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None, None
    
    # ì²« ë²ˆì§¸ íƒ€ê²Ÿ ë³€ìˆ˜ ì‚¬ìš©
    target = y.iloc[:, 0].dropna()
    X_target = X.loc[target.index]
    
    print(f"ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜: {y.columns[0]}")
    print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ ìˆ˜: {len(target)}")
    
    # 1) í†µê³„ì  íŠ¹ì„± ì„ íƒ (F-score)
    print("\n1) F-score ê¸°ë°˜ íŠ¹ì„± ì„ íƒ")
    selector_f = SelectKBest(score_func=f_regression, k=min(k, X_target.shape[1]))
    X_selected_f = selector_f.fit_transform(X_target, target)
    
    selected_features_f = np.array(feature_names)[selector_f.get_support()]
    scores_f = selector_f.scores_[selector_f.get_support()]
    
    print(f"   - ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(selected_features_f)}")
    print("   - ìƒìœ„ 5ê°œ íŠ¹ì„±:")
    for i, (feature, score) in enumerate(zip(selected_features_f[:5], scores_f[:5])):
        print(f"     {i+1}. {feature}: {score:.2f}")
    
    # 2) ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ
    print("\n2) ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ")
    selector_mi = SelectKBest(score_func=mutual_info_regression, k=min(k, X_target.shape[1]))
    X_selected_mi = selector_mi.fit_transform(X_target, target)
    
    selected_features_mi = np.array(feature_names)[selector_mi.get_support()]
    scores_mi = selector_mi.scores_[selector_mi.get_support()]
    
    print(f"   - ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(selected_features_mi)}")
    print("   - ìƒìœ„ 5ê°œ íŠ¹ì„±:")
    for i, (feature, score) in enumerate(zip(selected_features_mi[:5], scores_mi[:5])):
        print(f"     {i+1}. {feature}: {score:.3f}")
    
    # 3) Random Forest ì¤‘ìš”ë„ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ
    print("\n3) Random Forest ì¤‘ìš”ë„ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ")
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_target, target)
    
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    top_features_rf = importance_df.head(k)['feature'].values
    
    print(f"   - ìƒìœ„ {k}ê°œ íŠ¹ì„± ì„ íƒ")
    print("   - ìƒìœ„ 5ê°œ íŠ¹ì„±:")
    for i, (_, row) in enumerate(importance_df.head(5).iterrows()):
        print(f"     {i+1}. {row['feature']}: {row['importance']:.3f}")
    
    return {
        'f_score': (selected_features_f, X_selected_f),
        'mutual_info': (selected_features_mi, X_selected_mi),
        'random_forest': (top_features_rf, X_target[top_features_rf])
    }, importance_df

if 'original_X' in locals() and 'target_y' in locals():
    feature_selection_results, feature_importance = feature_selection(
        original_X, target_y, feature_names
    )

# =============================================================================
# 8. í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
# =============================================================================
print("\nğŸ¨ 8ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
print("-" * 50)

def clustering_analysis(X_scaled, max_clusters=8):
    """í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ í•¨ìˆ˜"""
    
    print("ğŸ” K-means í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
    
    # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (Elbow Method)
    inertias = []
    K_range = range(2, max_clusters + 1)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        inertias.append(kmeans.inertia_)
    
    # Elbow plot
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(K_range, inertias, 'bo-')
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('Inertia')
    plt.title('Elbow Method for Optimal k')
    plt.grid(True)
    
    # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    optimal_k = 4  # ì¼ë°˜ì ìœ¼ë¡œ 4-5ê°œê°€ ì ë‹¹
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    print(f"   - ì„ íƒëœ í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k}")
    print(f"   - ê° í´ëŸ¬ìŠ¤í„°ë³„ ìƒ˜í”Œ ìˆ˜:")
    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
    for cluster, count in cluster_counts.items():
        print(f"     í´ëŸ¬ìŠ¤í„° {cluster}: {count}ê°œ")
    
    # PCAë¡œ 2D ì‹œê°í™”
    if 'X_pca' in globals() and X_pca.shape[1] >= 2:
        plt.subplot(1, 2, 2)
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)
        plt.xlabel('First Principal Component')
        plt.ylabel('Second Principal Component')
        plt.title('Clusters in PCA Space')
        plt.colorbar(scatter)
    
    plt.tight_layout()
    plt.savefig('clustering_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return cluster_labels, kmeans

if 'scaled_datasets' in locals():
    cluster_labels, kmeans_model = clustering_analysis(scaled_datasets['StandardScaler'])

# =============================================================================
# 9. ì˜ˆì¸¡ ëª¨ë¸ë§ ë° ì„±ëŠ¥ í‰ê°€
# =============================================================================
print("\nğŸ¤– 9ë‹¨ê³„: ì˜ˆì¸¡ ëª¨ë¸ë§")
print("-" * 50)

def build_prediction_model(X, y, feature_names):
    """ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• ë° í‰ê°€ í•¨ìˆ˜"""
    
    if y is None or len(y.columns) == 0:
        print("âŒ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì—†ì–´ ëª¨ë¸ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    # ì²« ë²ˆì§¸ íƒ€ê²Ÿ ë³€ìˆ˜ ì‚¬ìš©
    target = y.iloc[:, 0].dropna()
    X_model = X.loc[target.index]
    
    print(f"ğŸ¯ ì˜ˆì¸¡ ëŒ€ìƒ: {y.columns[0]}")
    print(f"ğŸ“Š ëª¨ë¸ë§ ë°ì´í„°: {X_model.shape}")
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X_model, target, test_size=0.2, random_state=42
    )
    
    print(f"   - í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
    
    # Random Forest ëª¨ë¸ í›ˆë ¨
    rf_model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    
    rf_model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡ ë° í‰ê°€
    y_train_pred = rf_model.predict(X_train)
    y_test_pred = rf_model.predict(X_test)
    
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"\nğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   - í›ˆë ¨ RMSE: {train_rmse:.3f}")
    print(f"   - í…ŒìŠ¤íŠ¸ RMSE: {test_rmse:.3f}")
    print(f"   - í›ˆë ¨ RÂ²: {train_r2:.3f}")
    print(f"   - í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.3f}")
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False).head(15)
    
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(feature_importance)), feature_importance['importance'])
    plt.yticks(range(len(feature_importance)), feature_importance['feature'])
    plt.xlabel('Feature Importance')
    plt.title('Top 15 Most Important Features')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return rf_model, {
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'train_r2': train_r2,
        'test_r2': test_r2
    }

if 'original_X' in locals() and 'target_y' in locals():
    model, performance = build_prediction_model(original_X, target_y, feature_names)

# =============================================================================
# 10. ê²°ê³¼ ìš”ì•½ ë° ì‹œê°í™”
# =============================================================================
print("\nğŸ“‹ 10ë‹¨ê³„: ê²°ê³¼ ìš”ì•½")
print("-" * 50)

def create_summary_report():
    """ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    print("ğŸ“Š ì œì•½ ì œì¡° ë°ì´í„° ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸")
    print("=" * 60)
    
    if 'merged_data' in locals():
        print(f"ğŸ“ˆ ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
        print(f"   - ì´ ë°°ì¹˜ ìˆ˜: {len(merged_data)}")
        print(f"   - ì›ë³¸ ë³€ìˆ˜ ìˆ˜: {len(lab_data.columns) + len(process_data.columns)}")
        print(f"   - ì „ì²˜ë¦¬ í›„ ë³€ìˆ˜ ìˆ˜: {len(engineered_data.columns) if 'engineered_data' in locals() else 'N/A'}")
    
    if 'optimal_components' in locals():
        print(f"\nğŸ” ì°¨ì› ì¶•ì†Œ ê²°ê³¼:")
        print(f"   - 95% ë¶„ì‚° ì„¤ëª… ì£¼ì„±ë¶„ ìˆ˜: {optimal_components}")
        print(f"   - PCA ì ìš© í›„ ì°¨ì›: {X_pca.shape[1] if 'X_pca' in locals() else 'N/A'}")
    
    if 'cluster_labels' in locals():
        print(f"\nğŸ¨ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
        print(f"   - í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(np.unique(cluster_labels))}")
        print(f"   - ê° í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬: {dict(zip(*np.unique(cluster_labels, return_counts=True)))}")
    
    if 'performance' in locals():
        print(f"\nğŸ¤– ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   - í…ŒìŠ¤íŠ¸ RÂ²: {performance['test_r2']:.3f}")
        print(f"   - í…ŒìŠ¤íŠ¸ RMSE: {performance['test_rmse']:.3f}")
    
    print(f"\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    print(f"   1. ì œì•½ ì œì¡° ê³µì •ì—ì„œëŠ” ì••ì¶•ë ¥, ì†ë„, ì¶©ì „ ê¹Šì´ê°€ ì£¼ìš” í’ˆì§ˆ ê²°ì • ìš”ì¸")
    print(f"   2. ì›ë£Œ í’ˆì§ˆ(API, ë¶€í˜•ì œ)ê³¼ ìµœì¢… ì œí’ˆ í’ˆì§ˆ ê°„ ê°•í•œ ìƒê´€ê´€ê³„ ì¡´ì¬")
    print(f"   3. ê³µì • ì•ˆì •ì„± ì§€í‘œë¥¼ í†µí•œ ì‹¤ì‹œê°„ í’ˆì§ˆ ì˜ˆì¸¡ ê°€ëŠ¥ì„± í™•ì¸")
    print(f"   4. í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ë°°ì¹˜ë³„ ìƒì‚° íŒ¨í„´ êµ¬ë¶„ ê°€ëŠ¥")

# ìµœì¢… ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
create_summary_report()

print("\n" + "=" * 80)
print("ğŸ‰ ì œì•½ ì œì¡° ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤ìŠµ ì™„ë£Œ!")
print("=" * 80)

print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
print("   - pca_analysis.png: PCA ë¶„ì„ ê²°ê³¼")
print("   - clustering_analysis.png: í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ê²°ê³¼")  
print("   - feature_importance.png: íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")

print("\nğŸ” ì¶”ê°€ í•™ìŠµ ë°©í–¥:")
print("   1. ì‹œê³„ì—´ ë°ì´í„°(Process/*.csv)ë¥¼ í™œìš©í•œ LSTM ëª¨ë¸ë§")
print("   2. ì•™ìƒë¸” ëª¨ë¸ì„ í†µí•œ ì˜ˆì¸¡ ì„±ëŠ¥ í–¥ìƒ")
print("   3. ì‹¤ì‹œê°„ í’ˆì§ˆ ë°©ì¶œ(RTRT) ì‹œìŠ¤í…œ êµ¬í˜„")
print("   4. ê³µì • ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ëª©ì  ìµœì í™”")