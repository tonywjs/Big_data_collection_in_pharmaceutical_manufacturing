"""
ì œì•½ ì œì¡° ê³µì • ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ì‹¤ìŠµ

ì´ ì‹¤ìŠµì—ì„œëŠ” ì œì•½ ì œì¡° ê³µì •ì˜ ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤:
1. ì‹œê³„ì—´ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
2. ì‹œê³„ì—´ íŠ¹ì„± ë¶„ì„ (íŠ¸ë Œë“œ, ê³„ì ˆì„±, ì´ìƒê°’)
3. ì‹œê³„ì—´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
4. ì‹œê³„ì—´ í´ëŸ¬ìŠ¤í„°ë§
5. ì‹œê³„ì—´ ê¸°ë°˜ í’ˆì§ˆ ì˜ˆì¸¡ ëª¨ë¸ë§

ë°ì´í„°: 10ì´ˆ ê°„ê²© ê³µì • ì„¼ì„œ ë°ì´í„° (ë°°ì¹˜ë³„)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import TimeSeriesKMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# ì‹œê°í™” ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.style.use('default')
sns.set_palette("husl")

print("=" * 80)
print("ì œì•½ ì œì¡° ê³µì • ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ì‹¤ìŠµ")
print("=" * 80)

# =============================================================================
# 1. ì‹œê³„ì—´ ë°ì´í„° ë¡œë”©
# =============================================================================
print("\nğŸ“Š 1ë‹¨ê³„: ì‹œê³„ì—´ ë°ì´í„° ë¡œë”©")
print("-" * 50)

def load_time_series_data(batch_numbers=[1, 2, 3, 4, 5]):
    """ì—¬ëŸ¬ ë°°ì¹˜ì˜ ì‹œê³„ì—´ ë°ì´í„° ë¡œë”©"""
    
    all_timeseries = {}
    
    for batch_num in batch_numbers:
        try:
            file_path = f'Process/{batch_num}.csv'
            df = pd.read_csv(file_path, sep=';')
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp')
            
            all_timeseries[batch_num] = df
            print(f"âœ… ë°°ì¹˜ {batch_num} ë¡œë”© ì™„ë£Œ: {df.shape}")
            
        except FileNotFoundError:
            print(f"âŒ ë°°ì¹˜ {batch_num} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
    
    return all_timeseries

# ì‹œê³„ì—´ ë°ì´í„° ë¡œë”©
timeseries_data = load_time_series_data([1, 2, 3, 4, 5])

if timeseries_data:
    print(f"\nğŸ“ˆ ë¡œë”©ëœ ë°°ì¹˜ ìˆ˜: {len(timeseries_data)}")
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ ë°ì´í„° êµ¬ì¡° í™•ì¸
    first_batch = list(timeseries_data.values())[0]
    print(f"ğŸ“Š ì²« ë²ˆì§¸ ë°°ì¹˜ ë°ì´í„° êµ¬ì¡°:")
    print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(first_batch.columns)}")
    print(f"   - ë ˆì½”ë“œ ìˆ˜: {len(first_batch)}")
    print(f"   - ì‹œê°„ ë²”ìœ„: {first_batch['timestamp'].min()} ~ {first_batch['timestamp'].max()}")
    print(f"   - ì¸¡ì • ë³€ìˆ˜: {[col for col in first_batch.columns if col not in ['timestamp', 'campaign', 'batch', 'code']]}")

# =============================================================================
# 2. ì‹œê³„ì—´ ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”
# =============================================================================
print("\nğŸ“Š 2ë‹¨ê³„: ì‹œê³„ì—´ ë°ì´í„° íƒìƒ‰")
print("-" * 50)

def explore_timeseries(ts_data):
    """ì‹œê³„ì—´ ë°ì´í„° íƒìƒ‰ í•¨ìˆ˜"""
    
    # ì£¼ìš” ê³µì • ë³€ìˆ˜ë“¤
    key_variables = ['tbl_speed', 'main_comp', 'tbl_fill', 'SREL', 'stiffness', 'ejection']
    
    # ì—¬ëŸ¬ ë°°ì¹˜ì˜ ì£¼ìš” ë³€ìˆ˜ ì‹œê°í™”
    fig, axes = plt.subplots(3, 2, figsize=(15, 12))
    axes = axes.flatten()
    
    for i, var in enumerate(key_variables):
        ax = axes[i]
        
        for batch_id, df in ts_data.items():
            if var in df.columns:
                # ì‹œê°„ì„ ì‹œì‘ì ë¶€í„°ì˜ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜
                time_minutes = (df['timestamp'] - df['timestamp'].min()).dt.total_seconds() / 60
                ax.plot(time_minutes, df[var], label=f'Batch {batch_id}', alpha=0.7)
        
        ax.set_title(f'{var} ì‹œê³„ì—´')
        ax.set_xlabel('Time (minutes)')
        ax.set_ylabel(var)
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('timeseries_overview.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… ì‹œê³„ì—´ ê°œìš” ì‹œê°í™” ì™„ë£Œ")

if timeseries_data:
    explore_timeseries(timeseries_data)

# =============================================================================
# 3. ì‹œê³„ì—´ ì „ì²˜ë¦¬
# =============================================================================
print("\nğŸ”§ 3ë‹¨ê³„: ì‹œê³„ì—´ ì „ì²˜ë¦¬")
print("-" * 50)

def preprocess_timeseries(ts_data):
    """ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬"""
    
    processed_data = {}
    
    for batch_id, df in ts_data.items():
        df_clean = df.copy()
        
        print(f"\nğŸ§¹ ë°°ì¹˜ {batch_id} ì „ì²˜ë¦¬:")
        
        # 1) ê²°ì¸¡ê°’ ì²˜ë¦¬
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        initial_nulls = df_clean[numeric_cols].isnull().sum().sum()
        
        # ì„ í˜• ë³´ê°„ìœ¼ë¡œ ê²°ì¸¡ê°’ ì±„ìš°ê¸°
        df_clean[numeric_cols] = df_clean[numeric_cols].interpolate(method='linear')
        
        # ì—¬ì „íˆ ê²°ì¸¡ê°’ì´ ìˆìœ¼ë©´ ì „ë°©/í›„ë°© ì±„ìš°ê¸°
        df_clean[numeric_cols] = df_clean[numeric_cols].fillna(method='ffill').fillna(method='bfill')
        
        final_nulls = df_clean[numeric_cols].isnull().sum().sum()
        print(f"   - ê²°ì¸¡ê°’ ì²˜ë¦¬: {initial_nulls} â†’ {final_nulls}")
        
        # 2) ì´ìƒê°’ íƒì§€ ë° ì²˜ë¦¬ (Z-score ë°©ë²•)
        z_threshold = 3
        outliers_removed = 0
        
        for col in numeric_cols:
            if col in ['tbl_speed', 'main_comp', 'tbl_fill', 'SREL']:
                z_scores = np.abs(stats.zscore(df_clean[col].dropna()))
                outliers = df_clean[col][z_scores > z_threshold]
                
                if len(outliers) > 0:
                    # ì´ìƒê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
                    median_val = df_clean[col].median()
                    df_clean.loc[z_scores > z_threshold, col] = median_val
                    outliers_removed += len(outliers)
        
        print(f"   - ì´ìƒê°’ ì²˜ë¦¬: {outliers_removed}ê°œ ê°’ ìˆ˜ì •")
        
        # 3) ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ì¶”ê°€
        df_clean['hour'] = df_clean['timestamp'].dt.hour
        df_clean['day_of_week'] = df_clean['timestamp'].dt.dayofweek
        df_clean['time_from_start'] = (df_clean['timestamp'] - df_clean['timestamp'].min()).dt.total_seconds() / 3600  # hours
        
        # 4) ì´ë™ í‰ê·  ì¶”ê°€ (ë…¸ì´ì¦ˆ ì œê±°)
        window_size = 10  # 100ì´ˆ ì´ë™ í‰ê·  (10ê°œ í¬ì¸íŠ¸ * 10ì´ˆ)
        key_vars = ['tbl_speed', 'main_comp', 'tbl_fill', 'SREL']
        
        for var in key_vars:
            if var in df_clean.columns:
                df_clean[f'{var}_ma'] = df_clean[var].rolling(window=window_size, center=True).mean()
        
        processed_data[batch_id] = df_clean
        print(f"   âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {df_clean.shape}")
    
    return processed_data

if timeseries_data:
    processed_timeseries = preprocess_timeseries(timeseries_data)

# =============================================================================
# 4. ì‹œê³„ì—´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
# =============================================================================
print("\nâš™ï¸ 4ë‹¨ê³„: ì‹œê³„ì—´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§")
print("-" * 50)

def extract_timeseries_features(ts_data):
    """ì‹œê³„ì—´ì—ì„œ í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ"""
    
    features_list = []
    
    for batch_id, df in ts_data.items():
        batch_features = {'batch_id': batch_id}
        
        # ì£¼ìš” ê³µì • ë³€ìˆ˜ë“¤
        key_vars = ['tbl_speed', 'main_comp', 'tbl_fill', 'SREL', 'stiffness', 'ejection']
        
        for var in key_vars:
            if var in df.columns:
                series = df[var].dropna()
                
                if len(series) > 0:
                    # ê¸°ë³¸ í†µê³„ëŸ‰
                    batch_features[f'{var}_mean'] = series.mean()
                    batch_features[f'{var}_std'] = series.std()
                    batch_features[f'{var}_min'] = series.min()
                    batch_features[f'{var}_max'] = series.max()
                    batch_features[f'{var}_median'] = series.median()
                    batch_features[f'{var}_q25'] = series.quantile(0.25)
                    batch_features[f'{var}_q75'] = series.quantile(0.75)
                    
                    # ë³€ë™ì„± ì§€í‘œ
                    batch_features[f'{var}_cv'] = series.std() / series.mean() if series.mean() != 0 else 0
                    batch_features[f'{var}_range'] = series.max() - series.min()
                    
                    # íŠ¸ë Œë“œ ì§€í‘œ
                    if len(series) > 1:
                        x = np.arange(len(series))
                        slope, _, _, _, _ = stats.linregress(x, series)
                        batch_features[f'{var}_trend'] = slope
                    
                    # ì•ˆì •ì„± ì§€í‘œ (ì—°ì†ëœ ê°’ë“¤ì˜ ì°¨ì´)
                    diff_series = series.diff().dropna()
                    if len(diff_series) > 0:
                        batch_features[f'{var}_stability'] = diff_series.abs().mean()
                    
                    # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì„± (ë³€í™” ë¹ˆë„)
                    zero_crossings = np.sum(np.diff(np.sign(series - series.mean())) != 0)
                    batch_features[f'{var}_zero_crossings'] = zero_crossings
        
        # ê³µì • ë‹¨ê³„ë³„ íŠ¹ì„±
        # ìƒì‚° êµ¬ê°„ë§Œ í•„í„°ë§ (ì†ë„ > 0)
        production_mask = df['tbl_speed'] > 0
        production_data = df[production_mask]
        
        if len(production_data) > 0:
            batch_features['production_duration'] = len(production_data) * 10 / 60  # minutes
            batch_features['production_ratio'] = len(production_data) / len(df)
            
            # ìƒì‚° êµ¬ê°„ì˜ ì•ˆì •ì„±
            if 'main_comp' in production_data.columns:
                batch_features['production_compression_stability'] = production_data['main_comp'].std()
        
        features_list.append(batch_features)
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    features_df = pd.DataFrame(features_list)
    
    print(f"âœ… ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ:")
    print(f"   - ë°°ì¹˜ ìˆ˜: {len(features_df)}")
    print(f"   - ì¶”ì¶œëœ íŠ¹ì„± ìˆ˜: {len(features_df.columns) - 1}")  # batch_id ì œì™¸
    
    return features_df

if 'processed_timeseries' in locals():
    timeseries_features = extract_timeseries_features(processed_timeseries)
    print(f"\nğŸ“Š ì¶”ì¶œëœ íŠ¹ì„± ì˜ˆì‹œ:")
    print(timeseries_features.head())

# =============================================================================
# 5. ì‹œê³„ì—´ í´ëŸ¬ìŠ¤í„°ë§
# =============================================================================
print("\nğŸ¨ 5ë‹¨ê³„: ì‹œê³„ì—´ í´ëŸ¬ìŠ¤í„°ë§")
print("-" * 50)

def timeseries_clustering(ts_data, n_clusters=3):
    """ì‹œê³„ì—´ íŒ¨í„´ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§"""
    
    # ì£¼ìš” ë³€ìˆ˜ì˜ ì‹œê³„ì—´ì„ ë°°ì¹˜ë³„ë¡œ ì •ë ¬
    main_variable = 'tbl_speed'  # ì£¼ìš” ë¶„ì„ ë³€ìˆ˜
    
    # ëª¨ë“  ë°°ì¹˜ì˜ ë°ì´í„°ë¥¼ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶”ê¸° (ë¦¬ìƒ˜í”Œë§)
    resampled_series = []
    batch_ids = []
    
    target_length = 1000  # ëª©í‘œ ê¸¸ì´
    
    for batch_id, df in ts_data.items():
        if main_variable in df.columns:
            series = df[main_variable].dropna()
            
            if len(series) > 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                # ì„ í˜• ë³´ê°„ìœ¼ë¡œ ëª©í‘œ ê¸¸ì´ì— ë§ê²Œ ë¦¬ìƒ˜í”Œë§
                x_old = np.linspace(0, 1, len(series))
                x_new = np.linspace(0, 1, target_length)
                resampled = np.interp(x_new, x_old, series.values)
                
                resampled_series.append(resampled)
                batch_ids.append(batch_id)
    
    if len(resampled_series) < 2:
        print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    # í‘œì¤€í™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(resampled_series)
    
    # K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    print(f"âœ… ì‹œê³„ì—´ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ:")
    print(f"   - í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
    print(f"   - ê° í´ëŸ¬ìŠ¤í„°ë³„ ë°°ì¹˜ ìˆ˜:")
    
    for i in range(n_clusters):
        cluster_batches = [batch_ids[j] for j in range(len(batch_ids)) if cluster_labels[j] == i]
        print(f"     í´ëŸ¬ìŠ¤í„° {i}: {cluster_batches}")
    
    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 2, 1)
    for i, (batch_id, series) in enumerate(zip(batch_ids, resampled_series)):
        color = plt.cm.viridis(cluster_labels[i] / (n_clusters - 1))
        plt.plot(series, color=color, alpha=0.7, label=f'Batch {batch_id} (Cluster {cluster_labels[i]})')
    
    plt.title(f'{main_variable} ì‹œê³„ì—´ í´ëŸ¬ìŠ¤í„°ë§')
    plt.xlabel('Time Points')
    plt.ylabel(main_variable)
    plt.legend()
    
    # PCAë¡œ 2D ì‹œê°í™”
    plt.subplot(1, 2, 2)
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
    plt.title('ì‹œê³„ì—´ í´ëŸ¬ìŠ¤í„° (PCA)')
    plt.colorbar(scatter)
    
    # ë°°ì¹˜ ID í‘œì‹œ
    for i, batch_id in enumerate(batch_ids):
        plt.annotate(f'B{batch_id}', (X_pca[i, 0], X_pca[i, 1]), fontsize=8)
    
    plt.tight_layout()
    plt.savefig('timeseries_clustering.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return cluster_labels, batch_ids

if 'processed_timeseries' in locals():
    ts_clusters, ts_batch_ids = timeseries_clustering(processed_timeseries)

# =============================================================================
# 6. ì‹œê³„ì—´ ì´ìƒê°’ íƒì§€
# =============================================================================
print("\nğŸš¨ 6ë‹¨ê³„: ì‹œê³„ì—´ ì´ìƒê°’ íƒì§€")
print("-" * 50)

def detect_timeseries_anomalies(ts_data):
    """ì‹œê³„ì—´ì—ì„œ ì´ìƒê°’ íƒì§€"""
    
    anomalies_summary = {}
    
    for batch_id, df in ts_data.items():
        batch_anomalies = {}
        
        key_vars = ['tbl_speed', 'main_comp', 'tbl_fill', 'SREL']
        
        for var in key_vars:
            if var in df.columns:
                series = df[var].dropna()
                
                if len(series) > 10:
                    # 1) í†µê³„ì  ì´ìƒê°’ (IQR ë°©ë²•)
                    Q1 = series.quantile(0.25)
                    Q3 = series.quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    outliers_iqr = series[(series < lower_bound) | (series > upper_bound)]
                    
                    # 2) ë³€í™”ìœ¨ ê¸°ë°˜ ì´ìƒê°’
                    diff_series = series.diff().abs()
                    diff_threshold = diff_series.quantile(0.95)  # ìƒìœ„ 5%
                    rapid_changes = diff_series[diff_series > diff_threshold]
                    
                    batch_anomalies[var] = {
                        'statistical_outliers': len(outliers_iqr),
                        'rapid_changes': len(rapid_changes),
                        'outlier_ratio': len(outliers_iqr) / len(series),
                        'max_change': diff_series.max()
                    }
        
        anomalies_summary[batch_id] = batch_anomalies
        
        # ë°°ì¹˜ë³„ ì´ìƒê°’ ë¹„ìœ¨ ê³„ì‚°
        total_outliers = sum([anomalies_summary[batch_id][var]['statistical_outliers'] 
                             for var in anomalies_summary[batch_id]])
        total_points = len(df) * len(key_vars)
        
        print(f"ğŸ“Š ë°°ì¹˜ {batch_id} ì´ìƒê°’ ë¶„ì„:")
        print(f"   - ì „ì²´ ì´ìƒê°’ ë¹„ìœ¨: {total_outliers/total_points:.2%}")
        
        for var in batch_anomalies:
            anomaly_info = batch_anomalies[var]
            print(f"   - {var}: í†µê³„ì  ì´ìƒê°’ {anomaly_info['statistical_outliers']}ê°œ "
                  f"({anomaly_info['outlier_ratio']:.2%})")
    
    return anomalies_summary

if 'processed_timeseries' in locals():
    anomalies_results = detect_timeseries_anomalies(processed_timeseries)

# =============================================================================
# 7. ì‹œê³„ì—´-í’ˆì§ˆ ì—°ê´€ì„± ë¶„ì„
# =============================================================================
print("\nğŸ”¬ 7ë‹¨ê³„: ì‹œê³„ì—´-í’ˆì§ˆ ì—°ê´€ì„± ë¶„ì„")
print("-" * 50)

def analyze_timeseries_quality_relationship():
    """ì‹œê³„ì—´ íŠ¹ì„±ê³¼ í’ˆì§ˆ ì§€í‘œ ê°„ ì—°ê´€ì„± ë¶„ì„"""
    
    try:
        # Laboratory ë°ì´í„° ë¡œë”© (í’ˆì§ˆ ì§€í‘œ)
        lab_data = pd.read_csv('Laboratory.csv', sep=';')
        
        if 'timeseries_features' in locals():
            # ë°°ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
            merged_data = pd.merge(timeseries_features, lab_data, 
                                 left_on='batch_id', right_on='batch', how='inner')
            
            print(f"âœ… ì‹œê³„ì—´-í’ˆì§ˆ ë°ì´í„° ë³‘í•© ì™„ë£Œ: {merged_data.shape}")
            
            # ì£¼ìš” í’ˆì§ˆ ì§€í‘œ
            quality_vars = ['dissolution_av', 'dissolution_min', 'impurities_total']
            
            # ì‹œê³„ì—´ íŠ¹ì„±ê³¼ í’ˆì§ˆ ì§€í‘œ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
            timeseries_cols = [col for col in merged_data.columns 
                             if any(prefix in col for prefix in ['tbl_speed_', 'main_comp_', 'tbl_fill_', 'SREL_'])]
            
            correlation_results = {}
            
            for quality_var in quality_vars:
                if quality_var in merged_data.columns:
                    correlations = []
                    
                    for ts_col in timeseries_cols:
                        if ts_col in merged_data.columns:
                            corr = merged_data[ts_col].corr(merged_data[quality_var])
                            if not np.isnan(corr):
                                correlations.append((ts_col, abs(corr)))
                    
                    # ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                    correlations.sort(key=lambda x: x[1], reverse=True)
                    correlation_results[quality_var] = correlations[:10]  # ìƒìœ„ 10ê°œ
                    
                    print(f"\nğŸ“Š {quality_var}ì™€ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ì‹œê³„ì—´ íŠ¹ì„±:")
                    for i, (feature, corr) in enumerate(correlations[:5]):
                        print(f"   {i+1}. {feature}: {corr:.3f}")
            
            # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì‹œê°í™”
            if len(timeseries_cols) > 0 and len([q for q in quality_vars if q in merged_data.columns]) > 0:
                correlation_matrix = merged_data[timeseries_cols[:20] + 
                                               [q for q in quality_vars if q in merged_data.columns]].corr()
                
                plt.figure(figsize=(12, 8))
                mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
                sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                           square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
                plt.title('ì‹œê³„ì—´ íŠ¹ì„± - í’ˆì§ˆ ì§€í‘œ ìƒê´€ê´€ê³„')
                plt.tight_layout()
                plt.savefig('timeseries_quality_correlation.png', dpi=300, bbox_inches='tight')
                plt.show()
            
            return merged_data, correlation_results
        
    except FileNotFoundError:
        print("âŒ Laboratory.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

if 'timeseries_features' in locals():
    ts_quality_data, quality_correlations = analyze_timeseries_quality_relationship()

# =============================================================================
# 8. ì‹œê³„ì—´ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ë§
# =============================================================================
print("\nğŸ¤– 8ë‹¨ê³„: ì‹œê³„ì—´ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ë§")
print("-" * 50)

def build_timeseries_prediction_model():
    """ì‹œê³„ì—´ íŠ¹ì„±ì„ ì‚¬ìš©í•œ í’ˆì§ˆ ì˜ˆì¸¡ ëª¨ë¸"""
    
    if 'ts_quality_data' not in locals() or ts_quality_data is None:
        print("âŒ ì‹œê³„ì—´-í’ˆì§ˆ ë³‘í•© ë°ì´í„°ê°€ ì—†ì–´ ëª¨ë¸ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ì„ íƒ
    target_var = 'dissolution_av'
    if target_var not in ts_quality_data.columns:
        print(f"âŒ íƒ€ê²Ÿ ë³€ìˆ˜ {target_var}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì‹œê³„ì—´ íŠ¹ì„±ë§Œ ì„ íƒ
    feature_cols = [col for col in ts_quality_data.columns 
                   if any(prefix in col for prefix in ['tbl_speed_', 'main_comp_', 'tbl_fill_', 'SREL_', 'production_'])]
    
    X = ts_quality_data[feature_cols].dropna()
    y = ts_quality_data.loc[X.index, target_var]
    
    print(f"ğŸ¯ ì˜ˆì¸¡ ëŒ€ìƒ: {target_var}")
    print(f"ğŸ“Š ì‹œê³„ì—´ íŠ¹ì„± ìˆ˜: {len(feature_cols)}")
    print(f"ğŸ“Š ëª¨ë¸ë§ ë°ì´í„°: {X.shape}")
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Random Forest ëª¨ë¸ í›ˆë ¨
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf_model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡ ë° í‰ê°€
    y_train_pred = rf_model.predict(X_train)
    y_test_pred = rf_model.predict(X_test)
    
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"\nğŸ“ˆ ì‹œê³„ì—´ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥:")
    print(f"   - í›ˆë ¨ RMSE: {train_rmse:.3f}")
    print(f"   - í…ŒìŠ¤íŠ¸ RMSE: {test_rmse:.3f}")
    print(f"   - í›ˆë ¨ RÂ²: {train_r2:.3f}")
    print(f"   - í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.3f}")
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False).head(15)
    
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(feature_importance)), feature_importance['importance'])
    plt.yticks(range(len(feature_importance)), feature_importance['feature'])
    plt.xlabel('Feature Importance')
    plt.title('ì‹œê³„ì—´ íŠ¹ì„± ì¤‘ìš”ë„ (Top 15)')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('timeseries_feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return rf_model, {
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'feature_importance': feature_importance
    }

if 'ts_quality_data' in locals():
    ts_model, ts_performance = build_timeseries_prediction_model()

# =============================================================================
# 9. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜
# =============================================================================
print("\nğŸ“¡ 9ë‹¨ê³„: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜")
print("-" * 50)

def simulate_realtime_monitoring(ts_data, model=None):
    """ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜"""
    
    if not ts_data:
        print("âŒ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ì–´ ì‹œë®¬ë ˆì´ì…˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ì‹œë®¬ë ˆì´ì…˜
    batch_id = list(ts_data.keys())[0]
    df = ts_data[batch_id].copy()
    
    print(f"ğŸ“¡ ë°°ì¹˜ {batch_id} ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜")
    
    # ì‹¤ì‹œê°„ í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
    window_size = 50  # 500ì´ˆ (8.3ë¶„) ì´ë™ ìœˆë„ìš°
    
    monitoring_results = []
    
    for i in range(window_size, len(df), 10):  # 10ê°œì”© ê±´ë„ˆë›°ë©° ì‹œë®¬ë ˆì´ì…˜
        # í˜„ì¬ ìœˆë„ìš° ë°ì´í„°
        window_data = df.iloc[i-window_size:i]
        
        # ì‹¤ì‹œê°„ í†µê³„ ê³„ì‚°
        stats_dict = {
            'timestamp': df.iloc[i]['timestamp'],
            'time_minutes': (df.iloc[i]['timestamp'] - df.iloc[0]['timestamp']).total_seconds() / 60
        }
        
        key_vars = ['tbl_speed', 'main_comp', 'tbl_fill', 'SREL']
        
        for var in key_vars:
            if var in window_data.columns:
                series = window_data[var].dropna()
                if len(series) > 0:
                    stats_dict[f'{var}_mean'] = series.mean()
                    stats_dict[f'{var}_std'] = series.std()
                    stats_dict[f'{var}_trend'] = np.polyfit(range(len(series)), series, 1)[0]
        
        monitoring_results.append(stats_dict)
    
    # ê²°ê³¼ ì‹œê°í™”
    monitoring_df = pd.DataFrame(monitoring_results)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()
    
    for i, var in enumerate(['tbl_speed', 'main_comp', 'tbl_fill', 'SREL']):
        if f'{var}_mean' in monitoring_df.columns:
            ax = axes[i]
            
            # ì‹¤ì‹œê°„ í‰ê· ê°’
            ax.plot(monitoring_df['time_minutes'], monitoring_df[f'{var}_mean'], 
                   'b-', label='Real-time Mean', linewidth=2)
            
            # ì‹¤ì‹œê°„ í‘œì¤€í¸ì°¨ (ë¶ˆì•ˆì •ì„± ì§€í‘œ)
            ax2 = ax.twinx()
            ax2.plot(monitoring_df['time_minutes'], monitoring_df[f'{var}_std'], 
                    'r--', label='Real-time Std', alpha=0.7)
            
            ax.set_xlabel('Time (minutes)')
            ax.set_ylabel(f'{var} Mean', color='b')
            ax2.set_ylabel(f'{var} Std', color='r')
            ax.set_title(f'{var} ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§')
            ax.grid(True, alpha=0.3)
            
            # ë²”ë¡€
            lines1, labels1 = ax.get_legend_handles_labels()
            lines2, labels2 = ax2.get_legend_handles_labels()
            ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    
    plt.tight_layout()
    plt.savefig('realtime_monitoring_simulation.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ")
    print(f"   - ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸ ìˆ˜: {len(monitoring_results)}")
    
    return monitoring_df

if 'processed_timeseries' in locals():
    realtime_monitoring = simulate_realtime_monitoring(processed_timeseries)

# =============================================================================
# 10. ê²°ê³¼ ìš”ì•½
# =============================================================================
print("\nğŸ“‹ 10ë‹¨ê³„: ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
print("-" * 50)

def create_timeseries_summary():
    """ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
    
    print("ğŸ“Š ì œì•½ ì œì¡° ê³µì • ì‹œê³„ì—´ ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸")
    print("=" * 60)
    
    if 'timeseries_data' in locals():
        print(f"ğŸ“ˆ ì‹œê³„ì—´ ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
        print(f"   - ë¶„ì„ëœ ë°°ì¹˜ ìˆ˜: {len(timeseries_data)}")
        total_records = sum(len(df) for df in timeseries_data.values())
        print(f"   - ì´ ì‹œê³„ì—´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}")
        
        avg_duration = np.mean([
            (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600 
            for df in timeseries_data.values()
        ])
        print(f"   - í‰ê·  ë°°ì¹˜ ìƒì‚° ì‹œê°„: {avg_duration:.1f}ì‹œê°„")
    
    if 'timeseries_features' in locals():
        print(f"\nğŸ” íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼:")
        print(f"   - ì¶”ì¶œëœ ì‹œê³„ì—´ íŠ¹ì„± ìˆ˜: {len(timeseries_features.columns) - 1}")
    
    if 'ts_clusters' in locals() and ts_clusters is not None:
        print(f"\nğŸ¨ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
        unique_clusters = len(set(ts_clusters))
        print(f"   - ë°œê²¬ëœ ìƒì‚° íŒ¨í„´ ìˆ˜: {unique_clusters}")
    
    if 'ts_performance' in locals():
        print(f"\nğŸ¤– ì‹œê³„ì—´ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   - í…ŒìŠ¤íŠ¸ RÂ²: {ts_performance['test_r2']:.3f}")
        print(f"   - í…ŒìŠ¤íŠ¸ RMSE: {ts_performance['test_rmse']:.3f}")
    
    print(f"\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    print(f"   1. ì œì¡° ê³µì •ì˜ ì••ì¶•ë ¥ê³¼ ì†ë„ ë³€ë™ì´ í’ˆì§ˆì— ì§ì ‘ì  ì˜í–¥")
    print(f"   2. ìƒì‚° ì´ˆê¸° ë‹¨ê³„ì˜ ì•ˆì •ì„±ì´ ìµœì¢… í’ˆì§ˆ ì˜ˆì¸¡ì— ì¤‘ìš”")
    print(f"   3. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ í†µí•œ ì¡°ê¸° í’ˆì§ˆ ì´ìƒ íƒì§€ ê°€ëŠ¥")
    print(f"   4. ë°°ì¹˜ë³„ ìƒì‚° íŒ¨í„´ ë¶„ë¥˜ë¥¼ í†µí•œ ê³µì • ìµœì í™” ë°©í–¥ ì œì‹œ")

# ìµœì¢… ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
create_timeseries_summary()

print("\n" + "=" * 80)
print("ğŸ‰ ì œì•½ ì œì¡° ê³µì • ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ì‹¤ìŠµ ì™„ë£Œ!")
print("=" * 80)

print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
print("   - timeseries_overview.png: ì‹œê³„ì—´ ë°ì´í„° ê°œìš”")
print("   - timeseries_clustering.png: ì‹œê³„ì—´ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
print("   - timeseries_quality_correlation.png: ì‹œê³„ì—´-í’ˆì§ˆ ìƒê´€ê´€ê³„")
print("   - timeseries_feature_importance.png: ì‹œê³„ì—´ íŠ¹ì„± ì¤‘ìš”ë„")
print("   - realtime_monitoring_simulation.png: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜")

print("\nğŸ” ì¶”ê°€ ì—°êµ¬ ë°©í–¥:")
print("   1. LSTM/GRUë¥¼ í™œìš©í•œ ë”¥ëŸ¬ë‹ ì‹œê³„ì—´ ì˜ˆì¸¡")
print("   2. ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ì´ìƒê°’ íƒì§€ ì•Œê³ ë¦¬ì¦˜")
print("   3. ì‹¤ì‹œê°„ í’ˆì§ˆ ë°©ì¶œ(RTRT) ì‹œìŠ¤í…œ êµ¬í˜„")
print("   4. ë””ì§€í„¸ íŠ¸ìœˆ ê¸°ë°˜ ê³µì • ì‹œë®¬ë ˆì´ì…˜")